---
title: "Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models"
collection: publications
category: conferences
permalink: /publication/hi-tom
citation: "<b>Yinghui He</b>, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng" # I've changed it to author
excerpt: "‚ÄúThey don‚Äôt know that we know they know we know‚Äù ü§Ø ‚Äî Does GPT-4 have Higher-Order Theory of Mind?  Introducing üëã Hi-ToM: a benchmark pushing LLMs to their limits in higher-order ToM (3rd order & beyond). LLMs‚Äô performance declines drastically to near 0 üìâ on 3rd and 4th!"
date: 2023-10-25
venue: "Findings of the 2023 Conference on Empirical Methods in Natural Language Processing"
# slidesurl: "https://lit.eecs.umich.edu/Hi-ToM/"
paperurl: "https://arxiv.org/abs/2310.16755"

---

<figure>
  <img src="{{ site.baseurl }}/images/tom-wide.png" alt="Hi-ToM Figure 1">
</figure>

**_Abstract_**: Theory of Mind (ToM) is the ability to reason about one's own and others' mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.
